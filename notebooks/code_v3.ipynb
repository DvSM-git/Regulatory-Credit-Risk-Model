{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6591c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from optbinning import OptimalBinning\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from itertools import combinations\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35199df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns in Dataset: 151\n",
      "Total variables in Dictionary: 152\n",
      "Matches found: 150\n",
      "\n",
      "------------------------------\n",
      "Columns in DATA but not in Dictionary (1):\n",
      "['verification_status_joint']\n",
      "------------------------------\n",
      "Columns in DICTIONARY but not in Data (2):\n",
      "['nan', 'verified_status_joint']\n"
     ]
    }
   ],
   "source": [
    "# Setting file paths\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "# Loading data\n",
    "\n",
    "FILE_PATH_DATA = project_root / 'data' / 'raw' / 'accepted_2007_to_2018Q4.csv.gz'\n",
    "FILE_PATH_DICTIONARY = project_root / 'data' / 'raw'/ 'dictionary_approved_raw.csv' \n",
    "\n",
    "df = pd.read_csv(FILE_PATH_DATA, compression = 'gzip', low_memory = False)\n",
    "df_dictionary = pd.read_csv(FILE_PATH_DICTIONARY, usecols = [0,1], low_memory = False)\n",
    "\n",
    "# Comparing Data to Dictionary\n",
    "\n",
    "data_columns = set(df.columns)\n",
    "dictionary_columns = set(df_dictionary.iloc[:, 0].astype(str).str.strip())\n",
    "\n",
    "common = data_columns.intersection(dictionary_columns)\n",
    "in_data_only = data_columns - dictionary_columns\n",
    "in_dict_only = dictionary_columns - data_columns\n",
    "\n",
    "print(f\"Total columns in Dataset: {len(data_columns)}\")\n",
    "print(f\"Total variables in Dictionary: {len(dictionary_columns)}\")\n",
    "print(f\"Matches found: {len(common)}\\n\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Columns in DATA but not in Dictionary ({len(in_data_only)}):\")\n",
    "print(list(in_data_only))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Columns in DICTIONARY but not in Data ({len(in_dict_only)}):\")\n",
    "print(list(in_dict_only))\n",
    "\n",
    "# Make sure the names of the features match\n",
    "df_dictionary.iloc[:, 0] = df_dictionary.iloc[:, 0].replace('verified_status_joint', 'verification_status_joint')\n",
    "df_dictionary = df_dictionary.dropna()\n",
    "\n",
    "# Save cleaned dictionary\n",
    "FILE_PATH_DICTIONARY_CLEANED = project_root / 'data' / 'cleaned'/ 'dictionary_approved_cleaned.csv'\n",
    "\n",
    "df_dictionary.to_csv(FILE_PATH_DICTIONARY_CLEANED)\n",
    "\n",
    "FILE_PATH = project_root / 'data' / 'accepted_2007_to_2018Q4.csv.gz'\n",
    "\n",
    "cols_to_keep = [\n",
    "    # Target\n",
    "    'loan_status',\n",
    "\n",
    "    # Structural / Lender Features (Known at Origination)\n",
    "    'term', 'loan_amnt', 'initial_list_status', 'application_type', 'installment',\n",
    "\n",
    "    # Applicant Features (The core data)\n",
    "    'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
    "    'purpose', 'dti', 'delinq_2yrs', \n",
    "    'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util',\n",
    "    'total_acc', 'mort_acc', 'pub_rec_bankruptcies', 'mths_since_last_delinq',\n",
    "    'fico_range_low', 'fico_range_high',\n",
    "\n",
    "    # Dates\n",
    "    'issue_d', 'earliest_cr_line'\n",
    "]\n",
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a26da847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns present\n",
      "loan_amnt                                     float64\n",
      "installment                                   float64\n",
      "annual_inc                                    float64\n",
      "dti                                           float64\n",
      "delinq_2yrs                                   float64\n",
      "inq_last_6mths                                float64\n",
      "open_acc                                      float64\n",
      "pub_rec                                       float64\n",
      "revol_bal                                     float64\n",
      "revol_util                                    float64\n",
      "total_acc                                     float64\n",
      "mort_acc                                      float64\n",
      "pub_rec_bankruptcies                          float64\n",
      "mths_since_last_delinq                        float64\n",
      "fico_range_low                                float64\n",
      "fico_range_high                               float64\n",
      "issue_d                                datetime64[ns]\n",
      "earliest_cr_line                       datetime64[ns]\n",
      "target                                          int64\n",
      "term_rank                                       int64\n",
      "initial_list_status_f                           int64\n",
      "individual_application                          int64\n",
      "emp_length_years                              float64\n",
      "home_ownership_mortgage                         int64\n",
      "home_ownership_none                             int64\n",
      "home_ownership_other                            int64\n",
      "home_ownership_own                              int64\n",
      "home_ownership_rent                             int64\n",
      "purpose_credit_card                             int64\n",
      "purpose_debt_consolidation                      int64\n",
      "purpose_educational                             int64\n",
      "purpose_home_improvement                        int64\n",
      "purpose_house                                   int64\n",
      "purpose_major_purchase                          int64\n",
      "purpose_medical                                 int64\n",
      "purpose_moving                                  int64\n",
      "purpose_other                                   int64\n",
      "purpose_renewable_energy                        int64\n",
      "purpose_small_business                          int64\n",
      "purpose_vacation                                int64\n",
      "purpose_wedding                                 int64\n",
      "verification_status_source_verified             int64\n",
      "verification_status_verified                    int64\n",
      "credit_hist_months                            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Turning all variables into int or float usable types\n",
    "\n",
    "target_statuses = [\n",
    "    'Fully Paid', \n",
    "    'Charged Off', \n",
    "    'Default', \n",
    "    'Does not meet the credit policy. Status:Fully Paid',\n",
    "    'Does not meet the credit policy. Status:Charged Off'\n",
    "]\n",
    "\n",
    "df = df[df['loan_status'].isin(target_statuses)].copy()\n",
    "\n",
    "# target\n",
    "bad_indicators = [\n",
    "    'Charged Off', \n",
    "    'Default', \n",
    "    'Does not meet the credit policy. Status:Charged Off',\n",
    "]\n",
    "df['target'] = np.where(df['loan_status'].isin(bad_indicators), 1, 0)\n",
    "df = df.drop(columns=['loan_status'])\n",
    "\n",
    "term_map = {' 36 months': 1, ' 60 months': 2}\n",
    "df['term_rank'] = df['term'].map(term_map)\n",
    "df = df.drop(columns=['term'])\n",
    "\n",
    "initial_list_status_map = {'w': 0, 'f': 1}\n",
    "df['initial_list_status_f'] = df['initial_list_status'].map(initial_list_status_map)\n",
    "df = df.drop(columns=['initial_list_status'])\n",
    "\n",
    "application_type_map = {'Joint App': 0, 'Individual': 1}\n",
    "df['individual_application'] = df['application_type'].map(application_type_map)\n",
    "df = df.drop(columns=['application_type'])\n",
    "\n",
    "df['emp_length_years'] = df['emp_length'].str.replace('< 1', '0').str.extract(r'(\\d+)')\n",
    "df['emp_length_years'] = pd.to_numeric(df['emp_length_years'], errors='coerce')\n",
    "df = df.drop(columns=['emp_length'])\n",
    "\n",
    "dummy_cols = ['home_ownership', 'purpose', 'verification_status']\n",
    "df = pd.get_dummies(df, columns=dummy_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Engineering date to get time since first credit lite\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y')\n",
    "df['credit_hist_months'] = ((df['issue_d'] - df['earliest_cr_line']) / pd.Timedelta(days=30))\n",
    "\n",
    "# Normalise column names\n",
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "\n",
    "\n",
    "categorical_columns = [\n",
    "    'target',\n",
    "    'term_rank',\n",
    "    'initial_list_status_f',\n",
    "    'individual_application',\n",
    "    'emp_length_years',\n",
    "    'home_ownership_mortgage', \n",
    "    'home_ownership_none',\n",
    "    'home_ownership_other',\n",
    "    'home_ownership_own',\n",
    "    'home_ownership_rent',\n",
    "    'verification_status_source_verified',\n",
    "    'verification_status_verified',\n",
    "    'purpose_credit_card',\n",
    "    'purpose_debt_consolidation',\n",
    "    'purpose_educational',\n",
    "    'purpose_home_improvement',\n",
    "    'purpose_house',\n",
    "    'purpose_major_purchase',\n",
    "    'purpose_medical',\n",
    "    'purpose_moving',\n",
    "    'purpose_other',\n",
    "    'purpose_renewable_energy',\n",
    "    'purpose_small_business',\n",
    "    'purpose_vacation',\n",
    "    'purpose_wedding'\n",
    "]\n",
    "\n",
    "missing_categorical_cols = [col for col in categorical_columns if col not in df.columns]\n",
    "\n",
    "if missing_categorical_cols:\n",
    "    print(f\"Warning: The following columns were not found in df: {missing_categorical_cols}\")\n",
    "else:\n",
    "    print(\"All columns present\")\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21028d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration over 41 base features...\n",
      "Processed 100 pairs. Retained 10 features so far.\n",
      "Processed 200 pairs. Retained 10 features so far.\n",
      "Processed 300 pairs. Retained 10 features so far.\n",
      "Processed 400 pairs. Retained 10 features so far.\n",
      "Processed 500 pairs. Retained 19 features so far.\n",
      "Processed 600 pairs. Retained 29 features so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david_97mazrf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\david_97mazrf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 700 pairs. Retained 31 features so far.\n",
      "Processed 800 pairs. Retained 32 features so far.\n",
      "Final shape: (1348099, 76)\n"
     ]
    }
   ],
   "source": [
    "# Computing feasible interaction terms\n",
    "\n",
    "# Downcast to float32\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "# Setup\n",
    "target = df['target'].values # Ensure target is available\n",
    "non_predictive = ['target', 'issue_d', 'earliest_cr_line']\n",
    "features = [col for col in df.columns if col not in non_predictive]\n",
    "\n",
    "# Parameters\n",
    "CORR_THRESHOLD = 0.05\n",
    "new_features_list = []\n",
    "batch_count = 0\n",
    "\n",
    "print(f\"Starting iteration over {len(features)} base features...\")\n",
    "\n",
    "# Iterate, Compute, Screen, Discard\n",
    "for v1, v2 in combinations(features, 2):\n",
    "    col_v1 = df[v1].values\n",
    "    col_v2 = df[v2].values\n",
    "    \n",
    "    interactions = {}\n",
    "    \n",
    "    # Product\n",
    "    feat_mult = col_v1 * col_v2\n",
    "    interactions[f'{v1}_x_{v2}'] = feat_mult\n",
    "    \n",
    "    # Discarding those with low correlation to keep memory usage low\n",
    "    for name, data in interactions.items():\n",
    "        # Configuration\n",
    "        dtype = \"categorical\" if col in categorical_columns else \"numerical\"\n",
    "        \n",
    "        # Enforce Monotonicity for numerical variables\n",
    "        optb = OptimalBinning(\n",
    "            name=col, \n",
    "            dtype=dtype, \n",
    "            solver=\"cp\",\n",
    "            monotonic_trend=\"auto_asc_desc\",\n",
    "            min_bin_size = 0.05,\n",
    "            max_pvalue = 0.05\n",
    "        )\n",
    "\n",
    "        # Fit the Binner\n",
    "        try:\n",
    "            optb.fit(df_train[col], df_train[target_col])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {col} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Store Results\n",
    "        binning_process[col] = optb\n",
    "        \n",
    "        # Get IV\n",
    "        binning_table = optb.binning_table.build()\n",
    "        iv = binning_table.loc['Totals', 'IV']\n",
    "        iv_summary.append({'Variable': col, 'IV': iv})\n",
    "        \n",
    "        # Only keep if correlation exceeds threshold (\n",
    "        if abs(corr) > CORR_THRESHOLD:\n",
    "            # Store as Series to preserve index, append to list\n",
    "            new_features_list.append(pd.Series(data, index=df.index, name=name))\n",
    "\n",
    "    # Explicitly clear temporary variables and run garbage collection every 100 pairs\n",
    "    del col_v1, col_v2, interactions\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        gc.collect()\n",
    "        print(f\"Processed {batch_count} pairs. Retained {len(new_features_list)} features so far.\")\n",
    "\n",
    "# Construct Final DataFrame\n",
    "if new_features_list:\n",
    "    df_new = pd.concat(new_features_list, axis=1)\n",
    "    df = pd.concat([df, df_new], axis=1)\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"No features met the correlation threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f92298",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Lasso Estimation\u001b[39;00m\n\u001b[32m     17\u001b[39m lasso = LogisticRegressionCV(\n\u001b[32m     18\u001b[39m     Cs=\u001b[32m10\u001b[39m, \n\u001b[32m     19\u001b[39m     cv=\u001b[32m3\u001b[39m,                 \u001b[38;5;66;03m# 3-fold CV for speed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     tol=\u001b[32m1e-4\u001b[39m\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mlasso\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Feature Selection\u001b[39;00m\n\u001b[32m     32\u001b[39m coefs = lasso.coef_[\u001b[32m0\u001b[39m] \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:2035\u001b[39m, in \u001b[36mLogisticRegressionCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2033\u001b[39m     prefer = \u001b[33m\"\u001b[39m\u001b[33mprocesses\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2035\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdual\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2046\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2047\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2048\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2060\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_encoded_labels\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratios_\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[38;5;66;03m# _log_reg_scoring_path will output different shapes depending on the\u001b[39;00m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# multi_class param, so we need to reshape the outputs accordingly.\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2074\u001b[39m \u001b[38;5;66;03m#  (n_classes, n_folds, n_Cs . n_l1_ratios) or\u001b[39;00m\n\u001b[32m   2075\u001b[39m \u001b[38;5;66;03m#  (1, n_folds, n_Cs . n_l1_ratios)\u001b[39;00m\n\u001b[32m   2076\u001b[39m coefs_paths, Cs, scores, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fitting lasso regression to check which variables are useful\n",
    "\n",
    "non_predictive = ['target', 'issue_d', 'earliest_cr_line']\n",
    "drop_cols = [c for c in non_predictive if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=drop_cols)\n",
    "y = df['target']\n",
    "\n",
    "# Cleaning\n",
    "X = X.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Lasso Estimation\n",
    "lasso = LogisticRegressionCV(\n",
    "    Cs=10, \n",
    "    cv=3, \n",
    "    penalty='l1', \n",
    "    solver='saga', \n",
    "    scoring='roc_auc',\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Feature Selection\n",
    "coefs = lasso.coef_[0] \n",
    "selected_mask = coefs != 0\n",
    "selected_features = X.columns[selected_mask]\n",
    "\n",
    "print(f\"Optimal C value: {lasso.C_[0]}\")\n",
    "print(f\"Original feature count: {X.shape[1]}\")\n",
    "print(f\"Features selected by Lasso: {len(selected_features)}\")\n",
    "\n",
    "# Create Final Subset DataFrame\n",
    "df_lasso = df[selected_features.tolist() + ['target']].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
